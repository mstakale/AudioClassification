{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are opening the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the zip file\n",
    "tar_file = tarfile.open('wav.tgz', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an array with the order of wav files in the feat.npy array\n",
    "path = np.load('path.npy')\n",
    "# an array with Mel-frequency cepstral coefficients extracted from each wav file. \n",
    "feat = np.load('feat.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we put the different files together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the files path and feat\n",
    "zipped = zip(path, feat)\n",
    "zipped_dict = dict(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert train and test dataset into numpy array -> train.values, test.valeus\n",
    "# 2. Create a dictionary from 1.\n",
    "train_dict = dict(train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping between both dictionaries\n",
    "map_dict = [(k, zipped_dict[k], v) for k, v in train_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two empty lists before split\n",
    "y_train_bs = []\n",
    "X_train_bs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set\n",
    "for i in range (len(map_dict)):\n",
    "    y_train_bs.append(map_dict[i][2])\n",
    "    X_train_bs.append(map_dict[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model: K-NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create our first model. A K-nearest neighbours with the standard parameter settings, without cross validation, and with just the mean of the MFCC features. We will use this accuracy as the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94824, 99, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating empty numpy array for putting in the features in the right format\n",
    "X = np.zeros((len(X_train_bs), 99, 13))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the features in the array\n",
    "row = 0\n",
    "for i in X_train_bs:\n",
    "    X[row, 0:len(i),] = i\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94824, 99, 13)\n",
      "(94824, 13)\n"
     ]
    }
   ],
   "source": [
    "# Taking mean of axis 1 to get two dimensional data\n",
    "print(X.shape)\n",
    "X = X.mean(axis = 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the labels also in a numpy array\n",
    "y = np.array(y_train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75859, 13)\n",
      "(18965, 13)\n",
      "(75859,)\n",
      "(18965,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31231215396783546"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Model => Defining and running the model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature manipulation/engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve our accuracy we want do not just want to use the mean of of the MFCC, but use every piece of data we have. For this we flatten the three dimenstional dataframe to a two dimensional dataframe. Furthermore we scale our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list which contains horizontally distributed 99x13 features for each audio\n",
    "lis = []\n",
    "for i in range(X.shape[0]):\n",
    "    lis.append(X[i])\n",
    "lis_array = np.array(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the 99x13 into 1287\n",
    "M = []\n",
    "for i in range (len(lis)):\n",
    "    M.append(lis[i].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put zeros if the lenght is less than 1287\n",
    "M_pad = []\n",
    "for i in range (len(M)):\n",
    "    M_pad.append(np.pad(M[i], (0, 1287 - len(M[i])), 'constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the code above did what we want\n",
    "for i in range (len(M_pad)):\n",
    "    if len(M_pad[i]) != 1287:\n",
    "        print(len(M_pad[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertically stack the features of all audios\n",
    "M_pad_stack = []\n",
    "for i in range (len(M_pad)):\n",
    "    M_pad_stack.append(np.vstack(M_pad[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pad_stack_arr = np.array(M_pad_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = M_pad_stack_arr.mean(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94824, 1287)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and engineering for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(test.values)\n",
    "test_path = []\n",
    "for k in range (len(test_list)):\n",
    "    test_path.append(test_list[k][0])\n",
    "    \n",
    "map_test = [(k, zipped_dict[k]) for k in test_path]\n",
    "X_test = []\n",
    "\n",
    "for i in range (len(map_test)):\n",
    "    X_test.append(map_test[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array(X_test)\n",
    "\n",
    "# a list which contains horizontally distributed 99x13 features for each audio\n",
    "lis_T = []\n",
    "for i in range(T.shape[0]):\n",
    "    lis_T.append(T[i])\n",
    "lis_T_array = np.array(lis_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11005, 1287)\n"
     ]
    }
   ],
   "source": [
    "# flatten the 99x13 into 1287\n",
    "M = []\n",
    "for i in range (len(lis_T)):\n",
    "    M.append(lis_T[i].ravel())\n",
    "    \n",
    "# put zeros if the lenght is less than 1287\n",
    "M_pad = []\n",
    "for i in range (len(M)):\n",
    "    M_pad.append(np.pad(M[i], (0, 1287 - len(M[i])), 'constant'))\n",
    "    \n",
    "# Check if the code above did what we want\n",
    "for i in range (len(M_pad)):\n",
    "    if len(M_pad[i]) != 1287:\n",
    "        print(len(M_pad[i]))\n",
    "\n",
    "# vertically stack the features of all audios\n",
    "M_pad_stack = []\n",
    "for i in range (len(M_pad)):\n",
    "    M_pad_stack.append(np.vstack(M_pad[i]))\n",
    "    \n",
    "M_pad_stack_arr = np.array(M_pad_stack)\n",
    "\n",
    "T_features = M_pad_stack_arr.mean(axis = 2)\n",
    "\n",
    "print(T_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "T = scaler.fit_transform(T_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten ,LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv2D , GlobalAveragePooling2D,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "from datetime import datetime \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Categorical classes \n",
    "\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y_train_bs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75859, 1287)\n",
      "(18965, 1287)\n",
      "(75859, 35)\n",
      "(18965, 35)\n"
     ]
    }
   ],
   "source": [
    "# split the train data into train and validation. 20% of the train data is now val data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, yy, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape Traina and validation data for CNN\n",
    "num_rows = 13\n",
    "num_columns = 99\n",
    "num_channels = 1\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],  num_columns,num_rows, num_channels)\n",
    "X_val = X_val.reshape(X_val.shape[0], num_columns,num_rows, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference for CNN model -> \n",
    "\n",
    "def cnn_model():\n",
    "    M = Sequential()\n",
    "    M.add(Conv2D(filters=16, kernel_size=(3,3), input_shape=(99,13,1), activation='relu', padding=\"same\"))\n",
    "    M.add(MaxPooling2D(pool_size=1))\n",
    "    M.add(Dropout(0.2))\n",
    "\n",
    "    M.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "    M.add(MaxPooling2D(pool_size=2))\n",
    "    M.add(Dropout(0.2))\n",
    "\n",
    "    M.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "    M.add(MaxPooling2D(pool_size=2))\n",
    "    M.add(Dropout(0.2))\n",
    "\n",
    "    M.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "    M.add(MaxPooling2D(pool_size=2))\n",
    "    M.add(Dropout(0.2))\n",
    "\n",
    "    M.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "    M.add(MaxPooling2D(pool_size=1))\n",
    "    M.add(Dropout(0.2))\n",
    "    M.add(GlobalAveragePooling2D())\n",
    "\n",
    "    M.add(Dense(35, activation='softmax'))\n",
    "    M.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75859/75859 [==============================] - 345s 5ms/step - loss: 1.2063 - accuracy: 0.6534\n",
      "Epoch 2/20\n",
      "75859/75859 [==============================] - 355s 5ms/step - loss: 0.5352 - accuracy: 0.8439\n",
      "Epoch 3/20\n",
      "75859/75859 [==============================] - 397s 5ms/step - loss: 0.4331 - accuracy: 0.8721\n",
      "Epoch 4/20\n",
      "75859/75859 [==============================] - 385s 5ms/step - loss: 0.3807 - accuracy: 0.8873\n",
      "Epoch 5/20\n",
      "75859/75859 [==============================] - 381s 5ms/step - loss: 0.3465 - accuracy: 0.8971\n",
      "Epoch 6/20\n",
      "75859/75859 [==============================] - 384s 5ms/step - loss: 0.3225 - accuracy: 0.9022\n",
      "Epoch 7/20\n",
      "75859/75859 [==============================] - 392s 5ms/step - loss: 0.3067 - accuracy: 0.9093\n",
      "Epoch 8/20\n",
      "75859/75859 [==============================] - 379s 5ms/step - loss: 0.2934 - accuracy: 0.9115\n",
      "Epoch 9/20\n",
      "75859/75859 [==============================] - 419s 6ms/step - loss: 0.2826 - accuracy: 0.9147\n",
      "Epoch 10/20\n",
      "75859/75859 [==============================] - 403s 5ms/step - loss: 0.2699 - accuracy: 0.9179\n",
      "Epoch 11/20\n",
      "75859/75859 [==============================] - 378s 5ms/step - loss: 0.2609 - accuracy: 0.9214\n",
      "Epoch 12/20\n",
      "75859/75859 [==============================] - 372s 5ms/step - loss: 0.2553 - accuracy: 0.9228\n",
      "Epoch 13/20\n",
      "75859/75859 [==============================] - 362s 5ms/step - loss: 0.2496 - accuracy: 0.9247\n",
      "Epoch 14/20\n",
      "75859/75859 [==============================] - 370s 5ms/step - loss: 0.2459 - accuracy: 0.9250\n",
      "Epoch 15/20\n",
      "75859/75859 [==============================] - 375s 5ms/step - loss: 0.2388 - accuracy: 0.9284\n",
      "Epoch 16/20\n",
      "75859/75859 [==============================] - 391s 5ms/step - loss: 0.2346 - accuracy: 0.9286\n",
      "Epoch 17/20\n",
      "75859/75859 [==============================] - 387s 5ms/step - loss: 0.2297 - accuracy: 0.9302\n",
      "Epoch 18/20\n",
      "75859/75859 [==============================] - 384s 5ms/step - loss: 0.2262 - accuracy: 0.9305\n",
      "Epoch 19/20\n",
      "75859/75859 [==============================] - 374s 5ms/step - loss: 0.2262 - accuracy: 0.9314\n",
      "Epoch 20/20\n",
      "75859/75859 [==============================] - 352s 5ms/step - loss: 0.2206 - accuracy: 0.9325\n",
      "18965/18965 [==============================] - 18s 931us/step\n",
      "Model evaluation  [56.61002503716117, 0.10313735902309418]\n",
      "Epoch 1/20\n",
      "75859/75859 [==============================] - 363s 5ms/step - loss: 1.2854 - accuracy: 0.6246\n",
      "Epoch 2/20\n",
      "75859/75859 [==============================] - 366s 5ms/step - loss: 0.6241 - accuracy: 0.8131\n",
      "Epoch 3/20\n",
      "53440/75859 [====================>.........] - ETA: 1:53 - loss: 0.5167 - accuracy: 0.8437"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#reference for kfold validation -> https://androidkt.com/k-fold-cross-validation-with-tensorflow-keras/\n",
    " \n",
    "n_split=5\n",
    "num_rows = 13\n",
    "num_columns = 99\n",
    "num_channels = 1\n",
    "num_labels = yy.shape[1]\n",
    " \n",
    "for train_index,test_index in KFold(n_split).split(X):\n",
    "    X_train,x_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=yy[train_index],yy[test_index]\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0],  num_columns,num_rows, num_channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], num_columns,num_rows, num_channels)\n",
    "    M=cnn_model()\n",
    "    M.fit(X_train, y_train,epochs=20)\n",
    "    print('Model evaluation ',M.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section happens the following:  firstly the cnn is traint on all the training data. Secondly the predicitons are made on the test data and saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94824, 99, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "num_rows = 13\n",
    "num_columns = 99\n",
    "num_channels = 1\n",
    "\n",
    "X = X.reshape(X.shape[0],  num_columns,num_rows, num_channels)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = T.reshape(T.shape[0],num_columns,num_rows,num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5020c3ad31ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "y_test = M.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = le.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['word'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
